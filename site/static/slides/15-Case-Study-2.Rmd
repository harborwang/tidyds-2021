---
title: "Case Study"
subtitle: "Tidy Data Science with the Tidyverse and Tidymodels"
session: 15
author: W. Jake Thompson
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "assets/css/my-theme.css", "assets/css/my-fonts.css"]
    seal: false 
    lib_dir: libs
    nature:
      # autoplay: 5000
      highlightStyle: solarized-light
      highlightLanguage: ["r", "css", "yaml"]
      slideNumberFormat: "" 
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: [assets/header.html]
params:
  site_link: "https://bit.ly/tidyds-2021"
  class_link: "https://tidyds-2021.wjakethompson.com"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(collapse = TRUE,
                      fig.retina = 3,
                      fig.path = "images/case-study-2/plots/",
                      fig.align = "center",
                      fig.asp = 0.618,
                      comment = "#>")

xaringanExtra::use_share_again()
xaringanExtra::use_panelset()
xaringanExtra::use_extra_styles(hover_code_line = TRUE,
                                mute_unhighlighted_code = FALSE)
xaringanExtra::use_scribble(pen_color = "#009FB7")

yt_counter <- 0
library(countdown)
library(tidyverse)
library(tidymodels)
library(flair)
library(here)
library(knitr)
library(downlit)
library(vip)
library(themis)

library(xaringancolor)
blue <- "#009FB7"
light_blue <- "#0ADEFF"
yellow <- "#FED766"
dark_yellow <- "#A27A01"
pink <- "#CB297B"
light_pink <- "#FF8DC6"
green <- "#5FAD56"
dark_green <- "#3C6E35"
grey <- "#696773"

theme_set(wjake::theme_wjake(base_family = "Source Sans Pro",
                             base_size = 14,
                             axis_title_size = 12))
```


class: title-slide, center

<span class="fa-stack fa-4x">
  <i class="fa fa-circle fa-stack-2x" style="color: #ffffff;"></i>
  <strong class="fa-stack-1x" style="color:#009FB7;">`r rmarkdown::metadata$session`</strong>
</span> 

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle`

### `r rmarkdown::metadata$author`

#### [`r params$class_link`](`r params$class_link`) &#183; [`r params$site_link`](`r params$site_link`)

.footer-license[*Tidy Data Science with the Tidyverse and Tidymodels* is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).]

<div style = "position:fixed; visibility: hidden">
  $$\require{color}\definecolor{blue}{rgb}{0, 0.623529411764706, 0.717647058823529}$$
  $$\require{color}\definecolor{light_blue}{rgb}{0.0392156862745098, 0.870588235294118, 1}$$
  $$\require{color}\definecolor{yellow}{rgb}{0.996078431372549, 0.843137254901961, 0.4}$$
  $$\require{color}\definecolor{dark_yellow}{rgb}{0.635294117647059, 0.47843137254902, 0.00392156862745098}$$
  $$\require{color}\definecolor{pink}{rgb}{0.796078431372549, 0.16078431372549, 0.482352941176471}$$
  $$\require{color}\definecolor{light_pink}{rgb}{1, 0.552941176470588, 0.776470588235294}$$
  $$\require{color}\definecolor{grey}{rgb}{0.411764705882353, 0.403921568627451, 0.450980392156863}$$
</div>
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        blue: ["{\\color{blue}{#1}}", 1],
        light_blue: ["{\\color{light_blue}{#1}}", 1],
        yellow: ["{\\color{yellow}{#1}}", 1],
        dark_yellow: ["{\\color{dark_yellow}{#1}}", 1],
        pink: ["{\\color{pink}{#1}}", 1],
        light_pink: ["{\\color{light_pink}{#1}}", 1],
        grey: ["{\\color{grey}{#1}}", 1]
      },
      loader: {load: ['[tex]/color']},
      tex: {packages: {'[+]': ['color']}}
    }
  });
</script>


---
background-image: url(images/case-study-2/all-hex.png)
background-position: center middle
background-size: 70%

---
class: your-turn

# Your Turn 0

.big[
* Open the R Notebook **materials/exercises/15-case-study-2.Rmd**
* Run the setup chunk
]

```{r yt-setwd-cd, echo = FALSE}
countdown(minutes = 1, seconds = 0,
          font_size = "2em",
          color_border = yellow,
          color_background = blue,
          color_text = yellow,
          color_running_background = "#F0F0F0",
          color_running_text = blue,
          color_finished_background = yellow,
          color_finished_text = blue)
```

---
background-image: url(images/case-study-2/tm-process/tmprocess.001.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.002.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.003.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.004.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.005.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.006.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.007.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.008.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.009.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.010.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.011.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.012.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.013.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.014.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.015.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.016.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.017.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.018.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.019.png)
background-size: cover

---
# Hotel bookings

* Data from [Antonio, Almeida, & Nunes (2019)](https://doi.org/10.1016/j.dib.2018.11.126)

* We're using a slightly modified version that includes only *hotel stays*

--

.big[
* **Which hotel stays included children and/or babies?**
]

* Two methods:
    * Logistic regression
    * Random forest

.footnote[
Based on tidymodels [case study](https://www.tidymodels.org/start/case-study/).
]

???

In paper authors caution that distributions of some variables were different for canceled vs. not canceled. This makes sense because much of that information is gathered (or gathered again more accurately) when guests check in for their stay, so canceled bookings are likely to have more missing data than non-canceled bookings, and/or to have different characteristics when data is not missing

---
# `hotels`

```{r include = FALSE}
hotels <- 
  read_csv('https://tidymodels.org/start/case-study/hotels.csv') %>%
  mutate(across(where(is.character), as.factor))
```

```{r}
glimpse(hotels)
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Look at our outcome variable, `children`.

* What are the levels?

* What proportion of cases are in each level?

```{r echo = FALSE}
countdown(minutes = 3)
```

---
class: your-turn

```{r}
hotels %>%
  count(children) %>%
  mutate(prop = n / sum(n))
```

???

With such a large class imbalance, we may consider upsampling or downsampling. For now, let's use the data as is.

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create an initial split of our data to create a training and a testing set. Call the split `splits`.

Because of the class imbalance, stratify the split by our outcome variable, `children`.

Extract the training data as `hotel_other` and the testing data as `hotel_test`.

Keep `set.seed(123)` in your code!

```{r echo = FALSE}
countdown(minutes = 3)
```

---
class: your-turn

```{r}
set.seed(123)

splits <- initial_split(hotels, strata = children)
splits

hotel_other <- training(splits)
hotel_test <- testing(splits)
```

---
```{r}
hotel_other %>%
  count(children) %>%
  mutate(prop = n / sum(n))

hotel_test %>%
  count(children) %>%
  mutate(prop = n / sum(n))
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

How to choose a prediction model? Let's use resampling to evaluate our potential models.

Create a 10-fold cross validation, stratified by `children`. Call it `folds`. 

```{r echo = FALSE}
countdown(minutes = 2)
```

---
class: your-turn

```{r create-folds, cache = TRUE}
set.seed(789)

folds <- vfold_cv(hotel_other, v = 10, strata = children)
folds
```

---
# Model 1: Penalized logistic regression

* Can be estimated in R using the {glmnet} package.

* A *penalty* is applied to slope parameters to pull less relevant predictors toward zero.

* What size penalty should be used?


???

Similar to Lasso regression, where slopes can actually be set to 0 if there is a large enough penalty.

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Build a model specification for the penalized logistic regression, called `lr_mod`.

Define the model so that the `penalty` for the model can be tuned.

Use `glmnet` as the model engine.

```{r echo = FALSE}
countdown(minutes = 2)
```

---
class: your-turn

```{r}
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")
```

???

Mixture = 1 means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.

---
# Model 1 recipe

---
class: pop-quiz

# Pop quiz!

What date-based predictors might be useful related to arrival date?

--

Year, month, day

--

Holidays

---
# Model 1 recipe

```{r include = FALSE}
library(glue)
step_date <- glue("[`step_date()`]({autolink_url('recipes::step_date')})")
step_holiday <- glue("[`step_holiday()`]({autolink_url('recipes::step_holiday')})")
```


* Date-based predictors
    * `r step_date`
    * `r step_holiday`

* Convert factors to dummy coded variables

* Remove variables that have only 1 value

* Center and scale numeric variables

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Using the provided list of holidays, create a recipe, called `lr_recipe`, for the logistic regression model that:

1. Creates `year`, `month`, and `day` variables from `arrival_date`.
2. Creates holiday indicators based on the `arrival_date`.
3. Remove `arrival_date` (not necessary with other features).
4. Create dummy variables from all predictors that are factors.
5. Remove all variables have only one value.
6. Center and scale all predictors.

```{r echo = FALSE}
countdown(minutes = 4)
```


---
class: your-turn

```{r}
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create an `lr_workflow` from the model specification and recipe you just created.

```{r echo = FALSE}
countdown(minutes = 2)
```

---
class: your-turn

```{r}
lr_workflow <-
  workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(lr_recipe)
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Tune the logistic regression workflow. Use the provided grid of tuning parameter values.

Which penalty value provides the best area under the ROC curve?

Create a plot showing `penalty` on the x-axis and area under the ROC curve on the y-axis.

```{r echo = FALSE}
countdown(minutes = 5)
```

---
class: your-turn

.panelset[
.panel[.panel-name[Tune Workflow]
```{r tune-lr, cache = TRUE, dependson = "create-folds"}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_res <-
  lr_workflow %>%
  tune_grid(folds,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

lr_res %>%
  show_best()
```
]

.panel[.panel-name[Create Plot]
```{r lr-roc-tune, fig.show = "hide"}
lr_res %>%
  collect_metrics() %>%
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  labs(y = "Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())
```

]

.panel[.panel-name[Plot]
```{r show-lr-roc, ref.label = "lr-roc-tune", echo = FALSE, out.width = "80%"}
```
]
]

???

What does this show us? Performance is better at the smaller penalty values, suggesting that the majority of the predictors are important. We also see a steep drop in the area under the ROC curve towards the highest penalty values. This happens because a large enough penalty will remove all predictors from the model, and not surprisingly predictive accuracy plummets with no predictors in the model (recall that an ROC AUC value of 0.50 means that the model does no better than chance at predicting the correct class).

---
```{r}
top_models <- lr_res %>%
  show_best("roc_auc", n = 15) %>%
  arrange(penalty)
top_models
```

???

Our model performance seems to plateau at the smaller penalty values, so going by the `roc_auc` metric alone could lead us to multiple options for the “best” value for this hyperparameter.

---
# Which is best?

```{r}
lr_best <- lr_res %>%
  select_best()
lr_best
```

---
```{r lr-select-best, echo = FALSE, out.width = "90%"}
lr_res %>%
  collect_metrics() %>%
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  geom_vline(data = lr_best, aes(xintercept = penalty),
             linetype = "dashed", color = pink) +
  labs(y = "Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())
```

---

```{r include = FALSE}
lr_pick <- top_models %>%
  rowid_to_column() %>%
  mutate(mean = round(mean, digits = 3)) %>%
  slice_max(mean, n = 1) %>%
  slice_max(rowid)
```


```{r lr-select-pick, echo = FALSE, out.width = "90%"}
lr_res %>%
  collect_metrics() %>%
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  geom_vline(data = lr_best, aes(xintercept = penalty),
             linetype = "dashed", color = pink) +
  geom_vline(data = lr_pick, aes(xintercept = penalty),
             linetype = "solid", color = pink) +
  labs(y = "Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())
```

???

We may want to choose a penalty value further along the x-axis, closer to where we start to see the decline in model performance. For example, candidate model `r lr_pick$rowid` with a penalty value of `r round(lr_pick$penalty, 5)` has effectively the same performance as the numerically best model, but might eliminate more predictors. This penalty value is marked by the solid line above. In general, fewer irrelevant predictors is better. If performance is about the same, we’d prefer to choose a higher penalty value.

---
```{r}
lr_best <- lr_res %>%
  collect_metrics() %>%
  arrange(penalty) %>%
  slice(13)

lr_best
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create an ROC curve for the selected `penalty` value.

1. Use `collect_predictions()` with `parameters = lr_best` to only get predictions for our selected penalty value.

2. Use the predictions and `roc_curve()` to make the data for the curve.

3. Add an additional variable called `model` that has the value `"Logistic Regression"`. We'll need this later.

4. Plot the ROC curve.

```{r echo = FALSE}
countdown(minutes = 5)
```

---
class: your-turn

.panelset[
.panel[.panel-name[Code]
```{r lr-roc-auc, fig.asp = NA, fig.show = "hide"}
lr_auc <- lr_res %>%
  collect_predictions(parameters = lr_best) %>%
  roc_curve(truth = children, estimate = .pred_children) %>%
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```
]

.panel[.panel-name[Plot]
```{r ref.label = "lr-roc-auc", echo = FALSE, fig.asp = NA, out.width = "50%"}
```
]
]

???

ROC of `r lr_best$mean` is good, but maybe we can do better with a non-liner model. Let's try a random forest!

---
# Model 2: Random forest

* Several packages, but we'll use {ranger}.

* More flexible than logistic regression.

* Ensemble of many decision trees.

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Build a model specification for the random forest model, called `rf_mod`.

Define the model so that the `mtry` and `min_n` for the model can be tuned.

Use `ranger` as the model engine.

Remember to set the mode!

```{r echo = FALSE}
countdown(minutes = 2)
```

---
class: your-turn

```{r}
rf_mod <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger") %>%
  set_mode("classification")
```

---
# Model 2 recipe

* Random forests do not require dummy or normalized predictors.

* We still want our date-based features.

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Using the same list of holidays, create a recipe, called `rf_recipe`, for the random forest model that:

1. Creates `year`, `month`, and `day` variables from `arrival_date`.
2. Creates holiday indicators based on the `arrival_date`.
3. Remove `arrival_date` (not necessary with other features).

```{r echo = FALSE}
countdown(minutes = 2)
```


---
class: your-turn

```{r}
rf_recipe <-
  recipe(children ~ ., data = hotel_other) %>%
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date)
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create an `rf_workflow` from the model sepcification and the recipe you just created.

```{r echo = FALSE}
countdown(minutes = 2)
```

---
class: your-turn

```{r}
rf_workflow <- 
  workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_recipe)
```

---
# Tune the workflow

---
class: center middle inverse

# `r emo::ji("warning")` Warning `r emo::ji("warning")`

--

Tuning this model will take a **long** time (> 1 hour)

---
# Tune the workflow

```{r eval = FALSE}
set.seed(345)

rf_res <- rf_workflow %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

--

```{r eval = FALSE}
rf_res <- read_rds(here("materials", "data", "hotels-rf-tune.rds"))
```


---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Which `mtry` and `min_n` values provide the best area under the ROC curve?

```{r echo = FALSE}
countdown(minutes = 2)
```

---
class: your-turn

.panelset[
.panel[.panel-name[Tune Workflow]
```{r rf-tune, message = FALSE, warning = FALSE, cache = TRUE, dependson="create-folds"}
all_cores <- parallel::detectCores(logical = FALSE)

library(doParallel)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)

set.seed(345)

rf_res <- rf_workflow %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```
]

.panel[.panel-name[Results]
```{r}
rf_res %>%
  show_best(metric = "roc_auc")
```
]

.panel[.panel-name[Plot]
```{r rf-tune-plot, echo = FALSE, out.width = "80%"}
autoplot(rf_res)
```
]
]

???

Plotting the results of the tuning process highlights that both mtry (number of predictors at each node) and min_n (minimum number of data points required to keep splitting) should be fairly small to optimize performance. However, the range of the y-axis indicates that the model is very robust to the choice of these parameter values---all but one of the ROC AUC values are greater than 0.90.

---
```{r}
rf_best <- rf_res %>%
  select_best(metric = "roc_auc")

rf_best
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create an ROC curve for the selected `mtry` and `min_n` values.

1. Use `collect_predictions()` with `parameters = rf_best` to only get predictions for our selected penalty value.

2. Use the predictions and `roc_curve()` to make the data for the curve.

3. Add an additional variable called `model` that has the value `"Random Forest"`. We'll need this later.

4. Plot the ROC curve.

```{r echo = FALSE}
countdown(minutes = 3)
```


---
class: your-turn

.panelset[
.panel[.panel-name[Code]
```{r rf-roc-auc, fig.asp = NA, fig.show = "hide"}
rf_auc <- rf_res %>%
  collect_predictions(parameters = rf_best) %>%
  roc_curve(truth = children, estimate = .pred_children) %>%
  mutate(model = "Random Forest")

autoplot(rf_auc)
```
]

.panel[.panel-name[Plot]
```{r ref.label = "rf-roc-auc", echo = FALSE, fig.asp = NA, out.width = "50%"}
```
]
]

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Compare the area under the ROC curves for our selected logistic regression and random forest models.

1. Which model provides the best ROC AUC?

2. Plot both ROC curves together. Is one model uniformly better?

Which model should we select as our final model?

```{r echo = FALSE}
countdown(minutes = 3)
```

---
class: your-turn

.panelset[
.panel[.panel-name[Code]
```{r rocauc-compare, out.width = "50%", fig.asp = NA, fig.show = "hide"}
lr_best

show_best(rf_res, n = 1)

bind_rows(lr_auc, rf_auc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line(size = 1.5, alpha = 0.8) +
  geom_abline(linetype = "dotted") +
  scale_color_viridis_d(option = "plasma", end = 0.6) +
  coord_equal()
```
]

.panel[.panel-name[Plot]
```{r echo = FALSE, out.width = "50%"}
include_graphics(fig_chunk("rocauc-compare", "png"))
```
]
]

---
class: center middle inverse

# Now what?

---
background-image: url(images/case-study-2/tm-process/tmprocess.007.png)
background-size: cover

---
background-image: url(images/case-study-2/tm-process/tmprocess.010.png)
background-size: cover

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create a new random forest model specification that uses our tuned values of `mtry` and `min_n`. Use `ranger` as the engine, with `importance = "impurity"`.

Create a new workflow, called `last_rf_workflow`, that is based on our original random forest workflow, but updated with the new model specification.

```{r echo = FALSE}
countdown(minutes = 2)
```

---
class: your-turn

```{r}
rf_best

last_rf_mod <-
  rand_forest(mtry = 7, min_n = 11, trees = 1000) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

last_rf_workflow <-
  rf_workflow %>%
  update_model(last_rf_mod)
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Using our final workflow, fit the random forest model to our entire training set. Then using predictions from the test set, find the area under the ROC curve for our final model.

```{r echo = FALSE}
countdown(minutes = 5)
```

---
class: your-turn

```{r last-rf-fit, cache = TRUE}
set.seed(345)

last_rf_fit <-
  last_rf_workflow %>%
  last_fit(splits) #<<

last_rf_fit %>%
  collect_metrics()
```

---
class: center middle inverse

# final looks

---
# Performance Comparison

```{r}
show_best(rf_res, metric = "roc_auc", n = 1)

collect_metrics(last_rf_fit)
```

---
.panelset[
```{r last-roc, panelset = c(source = "ROC Curve", output = "Plot"), out.width = "50%", fig.asp = NA}
last_rf_fit %>%
  collect_predictions() %>%
  roc_curve(children, .pred_children) %>%
  mutate(model = "Final") %>%
  bind_rows(rf_auc, lr_auc) %>%
  mutate(model = factor(model, levels = c("Logistic Regression", "Random Forest", "Final"))) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line(size = 1.5, alpha = 0.8) +
  geom_abline(linetype = "dotted") +
  scale_color_manual(values = c("Random Forest" = light_pink,
                                "Logistic Regression" = light_blue,
                                "Final" = pink)) +
  coord_equal()
```
]

---
.panelset[
```{r last-vip, panelset = c(source = "Variable Importance", output = "Plot"), out.width = "80%"}
last_rf_fit %>%
  pluck(".workflow", 1) %>%
  pull_workflow_fit() %>%
  vip(num_features = 20)
```
]

---
class: title-slide, center

# `r rmarkdown::metadata$title`

```{r closing-hex, echo = FALSE, out.width = "20%"}
include_graphics("images/hex/tidymodels.png")
```

## `r rmarkdown::metadata$subtitle`

### `r rmarkdown::metadata$author`

#### [`r params$class_link`](`r params$class_link`) &#183; [`r params$site_link`](`r params$site_link`)

.footer-license[*Tidy Data Science with the Tidyverse and Tidymodels* is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).]
