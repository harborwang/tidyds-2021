---
title: "Ensembling"
subtitle: "Tidy Data Science with the Tidyverse and Tidymodels"
session: 11
author: W. Jake Thompson
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "assets/css/my-theme.css", "assets/css/my-fonts.css"]
    seal: false 
    lib_dir: libs
    nature:
      # autoplay: 5000
      highlightStyle: solarized-light
      highlightLanguage: ["r", "css", "yaml"]
      slideNumberFormat: "" 
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "https://platform.twitter.com/widgets.js"
    includes:
      in_header: [assets/header.html]
params:
  site_link: "https://bit.ly/tidyds-2021"
  class_link: "https://tidyds-2021.wjakethompson.com"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(collapse = TRUE,
                      fig.retina = 3,
                      fig.path = "images/ensembling/plots/",
                      fig.align = "center",
                      fig.asp = 0.618,
                      comment = "#>")

xaringanExtra::use_share_again()
xaringanExtra::use_panelset()
xaringanExtra::use_extra_styles(hover_code_line = TRUE,
                                mute_unhighlighted_code = FALSE)
xaringanExtra::use_scribble(pen_color = "#009FB7")

yt_counter <- 0
library(countdown)
library(tidyverse)
library(tidymodels)
library(flair)
library(here)
library(knitr)
library(downlit)
library(vip)

library(xaringancolor)
blue <- "#009FB7"
light_blue <- "#0ADEFF"
yellow <- "#FED766"
dark_yellow <- "#A27A01"
pink <- "#CB297B"
light_pink <- "#FF8DC6"
green <- "#5FAD56"
dark_green <- "#3C6E35"
grey <- "#696773"

library(AmesHousing)
ames <- make_ames()

theme_set(wjake::theme_wjake(base_family = "Source Sans Pro",
                             base_size = 14,
                             axis_title_size = 12))
```

```{r read-data, include = FALSE}
# read in the data
stackoverflow <- read_rds(here::here("materials/data/stackoverflow.rds"))

# split the data
set.seed(100) # Important!
so_split <- initial_split(stackoverflow, strata = remote)
so_train <- training(so_split)
so_test  <- testing(so_split)
```

```{r plot-colors, include = FALSE}
# for figures
train_color <- yellow
test_color  <- green
data_color  <- blue
assess_color <- light_blue
splits_pal <- c(data_color, train_color, test_color)
```


class: title-slide, center

<span class="fa-stack fa-4x">
  <i class="fa fa-circle fa-stack-2x" style="color: #ffffff;"></i>
  <strong class="fa-stack-1x" style="color:#009FB7;">`r rmarkdown::metadata$session`</strong>
</span> 

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle`

### `r rmarkdown::metadata$author`

#### [`r params$class_link`](`r params$class_link`) &#183; [`r params$site_link`](`r params$site_link`)

.footer-license[*Tidy Data Science with the Tidyverse and Tidymodels* is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).]

<div style = "position:fixed; visibility: hidden">
  $$\require{color}\definecolor{blue}{rgb}{0, 0.623529411764706, 0.717647058823529}$$
  $$\require{color}\definecolor{light_blue}{rgb}{0.0392156862745098, 0.870588235294118, 1}$$
  $$\require{color}\definecolor{yellow}{rgb}{0.996078431372549, 0.843137254901961, 0.4}$$
  $$\require{color}\definecolor{dark_yellow}{rgb}{0.635294117647059, 0.47843137254902, 0.00392156862745098}$$
  $$\require{color}\definecolor{pink}{rgb}{0.796078431372549, 0.16078431372549, 0.482352941176471}$$
  $$\require{color}\definecolor{light_pink}{rgb}{1, 0.552941176470588, 0.776470588235294}$$
  $$\require{color}\definecolor{grey}{rgb}{0.411764705882353, 0.403921568627451, 0.450980392156863}$$
</div>
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        blue: ["{\\color{blue}{#1}}", 1],
        light_blue: ["{\\color{light_blue}{#1}}", 1],
        yellow: ["{\\color{yellow}{#1}}", 1],
        dark_yellow: ["{\\color{dark_yellow}{#1}}", 1],
        pink: ["{\\color{pink}{#1}}", 1],
        light_pink: ["{\\color{light_pink}{#1}}", 1],
        grey: ["{\\color{grey}{#1}}", 1]
      },
      loader: {load: ['[tex]/color']},
      tex: {packages: {'[+]': ['color']}}
    }
  });
</script>

---
class: your-turn

# Your Turn 0

.big[
* Open the R Notebook **materials/exercises/11-ensembling.Rmd**
* Run the setup chunk
]

```{r yt-setwd-cd, echo = FALSE}
countdown(minutes = 1, seconds = 0,
          font_size = "2em",
          color_border = yellow,
          color_background = blue,
          color_text = yellow,
          color_running_background = "#F0F0F0",
          color_running_text = blue,
          color_finished_background = yellow,
          color_finished_text = blue)
```

---
class: center
background-image: url("images/ensembling/tm-hex0.png")
background-position: center 70%
background-size: 90%

# tidymodels

---
class: center
background-image: url("images/ensembling/tm-hex1.png")
background-position: center 70%
background-size: 90%

# tidymodels

---
class: center
background-image: url("images/ensembling/tm-hex2.png")
background-position: center 70%
background-size: 90%

# tidymodels

---
class: center
background-image: url("images/ensembling/tm-hex3.png")
background-position: center 70%
background-size: 90%

# tidymodels

---
class: center
background-image: url("images/ensembling/tm-hex4.png")
background-position: center 70%
background-size: 90%

# tidymodels

---
class: center middle, frame


# To specify a model with parsnip

1\. Pick a .display[model]

2\. Set the .display[engine]

3\. Set the .display[mode] (if needed)

---
class: middle, frame

# .center[To specify a classification tree with parsnip]

```{r results='hide'}
decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Here is our very-vanilla parsnip model specification for a decision tree (also in your Rmd)...

```{r}
vanilla_tree_spec <-
  decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

---
class: your-turn

# Your turn `r yt_counter`

Fill in the blanks to return the accuracy and ROC AUC for this model.

```{r echo=FALSE}
countdown(minutes = 2)
```

---
class: your-turn

```{r}
set.seed(100)
so_folds <- vfold_cv(so_train, strata = remote)

dt_mod <- fit_resamples(vanilla_tree_spec,
                        remote ~ .,
                        resamples = so_folds)

dt_preds <- dt_mod %>%
  collect_metrics()
```

```{r include = FALSE}
vt_metrics <- dt_preds %>%
  select(.metric, .estimator, mean)
```


---
# `args()`

.big[Print the arguments for **parsnip** model specification.]

```{r eval = FALSE}
args(decision_tree)
```

---
# `decision_tree()`

.big[Specifies a decision tree model]

```{r eval = FALSE}
decision_tree(tree_depth = 30, min_n = 20, cost_complexity = .01)
```

--

*either* mode works!

---
# `decision_tree()`

.big[Specifies a decision tree model]

```{r eval = FALSE}
decision_tree(
  tree_depth = 30,       # max tree depth
  min_n = 20,            # smallest node allowed
  cost_complexity = .01  # 0 > cp > 0.1
)
```

---
# `set_args()`

.big[Change the arguments for a **parsnip** model specification.]

```{r eval = FALSE}
_spec %>% set_args(tree_depth = 3)
```

---
```{r}
decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  set_args(tree_depth = 3) #<<
```

---
```{r}
decision_tree(tree_depth = 3) %>% #<<
  set_engine("rpart") %>%
  set_mode("classification")
```

---
# `tree_depth`

.big[
Cap the maximum tree depth.

A method to stop the tree early. Used to prevent overfitting.
]

```{r}
vanilla_tree_spec %>% set_args(tree_depth = 30)
```

---
exclude: true

```{r include=FALSE}
big_tree_spec <- 
  decision_tree(min_n = 1, cost_complexity = 0) %>% #<<
  set_engine("rpart") %>% 
  set_mode("classification")

big_tree <-
  last_fit(big_tree_spec,
           remote ~ ., 
           split = so_split) 

big_tree_cp <- big_tree %>%
  pluck(".workflow", 1) %>% 
  pull_workflow_fit() %>%
  .[["fit"]] %>%
  .[["cptable"]] %>%
  as_tibble() %>% 
  janitor::clean_names() %>% 
  pivot_longer(contains("error"), names_to = "error_type", values_to = "error_val") %>% 
  mutate(cp_round = round(cp, 4),
    cp_fct = as_factor(cp_round))
```

---
```{r splits-train-error, echo=FALSE, fig.align = "center", fig.width = 14, out.width = "90%"}
big_tree_cp %>% 
  filter(error_type == "rel_error") %>% 
  ggplot(aes(x = as.factor(nsplit), y = error_val, group = error_type, color =error_type)) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "number of splits", y = "error", color = NULL) +
  scale_color_manual(values = splits_pal[3], 
                     labels = "Training") +
  coord_cartesian(ylim = c(0, 1.08), expand = TRUE)
```

---
```{r cp-train-error, echo=FALSE, fig.align = "center", fig.width = 14, out.width = "90%"}
big_tree_cp %>% 
  filter(error_type == "rel_error") %>% 
  ggplot(aes(x = fct_rev(cp_fct), y = error_val, 
                        group = error_type, color = fct_rev(error_type))) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "cost complexity", y = "error", color = NULL) +
  scale_color_manual(values = splits_pal[3], 
                     labels = "Training") +
  scale_x_discrete() +
  coord_cartesian(ylim = c(0, 1.08), expand = TRUE)
```

---
```{r cp-test-error, echo=FALSE, fig.align = "center", fig.width = 14, out.width = "90%"}
ggplot(big_tree_cp, aes(x = fct_rev(cp_fct), y = error_val, 
                        group = error_type, color = fct_rev(error_type))) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "cost complexity", y = "error", color = NULL) +
  scale_color_manual(values = splits_pal[c(1, 3)], 
                     labels = c("Testing", "Training")) +
  scale_x_discrete() +
  coord_cartesian(ylim = c(0, 1.08), expand = TRUE)
```

---
# `min_n`

.big[
Set minimum `n` to split at any node.

Another early stopping method. Used to prevent overfitting.
]

```{r eval = FALSE}
vanilla_tree_spec %>% set_args(min_n = 20)
```

---
class: pop-quiz

# Pop quiz!

.big[What value of `min_n` would lead to the *most overfit* tree?]

--

`min_n` = 1

---
class: middle, center, frame

# Recap: early stopping

| `parsnip` arg | `rpart` arg | default | overfit? |
|---------------|-------------|:-------:|:--------:|
| `tree_depth`  | `maxdepth`  |    30   |`r emo::ji("up_arrow")`|
| `min_n`       | `minsplit`  |    20   |`r emo::ji("down_arrow")`|

---
# `cost_complexity`

.big[
Adds a cost or penalty to error rates of more complex trees.

A way to prune a tree. Used to prevent overfitting.
]

```{r}
vanilla_tree_spec %>% set_args(cost_complexity = .01)
```

--

.center[
Closer to zero `r emo::ji("right_arrow")` larger trees. 

Higher penalty `r emo::ji("right_arrow")` smaller trees. 
]

---

```{r cp-test-error2, echo=FALSE, fig.align = "center", fig.width = 14, out.width = "90%"}
<<cp-test-error>>
```

---
name: bonsai
background-image: url(images/ensembling/kari-shea-AVqh83jStMA-unsplash.jpg)
background-position: left
background-size: contain
class: middle

---
template: bonsai

.pull-right[

# Consider the bonsai

1. Small pot

1. Strong shears

]

---
template: bonsai

.pull-right[

# Consider the bonsai

1. ~~Small pot~~ .display[Early stopping]

1. ~~Strong shears~~ .display[Pruning]

]

---
class: middle, center, frame

# Recap: early stopping & pruning

| `parsnip` arg | `rpart` arg | default | overfit? |
|---------------|-------------|:-------:|:--------:|
| `tree_depth`  | `maxdepth`  |    30   |`r emo::ji("up_arrow")`|
| `min_n`       | `minsplit`  |    20   |`r emo::ji("down_arrow")`|
| `cost_complexity`  | `cp`  |    .01  |`r emo::ji("down_arrow")`|

---
class: middle, center

```{r echo=FALSE}
parsnip::get_model_env() %>% 
  pluck("decision_tree_args") %>% 
  filter(engine == "rpart") %>% 
  select(engine, parsnip, original) %>% 
  knitr::kable('html')
```

<https://rdrr.io/cran/rpart/man/rpart.control.html>

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create a new classification tree model spec; call it `big_tree_spec`. Set the cost complexity to `0`, and the minimum number of data points in a node to split to be `1`.

Compare the metrics of the big tree to the vanilla tree. Which one predicts the test set better?

```{r echo = FALSE}
countdown(minutes = 4)
```

---
class: your-turn

.panelset[
.panel[.panel-name[Code]
```{r}
big_tree_spec <-
  decision_tree(min_n = 1, cost_complexity = 0) %>% #<<
  set_engine("rpart") %>%
  set_mode("classification")

set.seed(100) # Important!
big_dt_mod <- fit_resamples(big_tree_spec,
                            remote ~ .,
                            resamples = so_folds)

big_dt_preds <- big_dt_mod %>%
  collect_metrics()
```
]

.panel[.panel-name[Metrics]
```{r}
big_dt_preds
```

```{r include = FALSE}
bt_metrics <- big_dt_preds %>%
  select(.metric, .estimator, mean)
```

Compare to `vanilla`: accuracy = `r round(vt_metrics$mean[[1]], 2)`; ROC AUC = `r round(vt_metrics$mean[[2]], 2)`

]
]

---
exclude: true

```{r bootstrap-tree, include=FALSE}
get_boot_trees <- function(seed = 1, tree_depth = 4) {
  # Make recipe
  so_rec <- 
    recipe(remote ~ ., 
           data = stackoverflow) 
  
  # Make learner
  tmp_tree_lnr <-
    decision_tree(tree_depth = tree_depth) %>%         
    set_engine("rpart", model = TRUE) %>%      
    set_mode("classification")
  
  # Make workflow
  temp_flow <- 
    workflow() %>% 
    add_model(tmp_tree_lnr) %>% 
    add_recipe(so_rec) 
  
  # Begin resampling
  set.seed(seed)
  so_boots <- so_train %>% 
    bootstraps(times = 1) %>% 
    pluck("splits", 1)
  
  boot_fit <- temp_flow %>% 
    fit(data = analysis(so_boots)) %>% 
    pull_workflow_fit() %>% 
    pluck("fit")
  
  boot_fit
}
```

```{r bootstrap-predict, include=FALSE}
get_boot_votes <- function(seed = 1, team = 1) {
  tree <- get_boot_trees(seed)
  mini_test <- so_test %>% 
    ungroup() %>% 
    mutate(obs = row_number()) %>% 
    group_by(remote) %>% 
    slice(team)
  preds <- 
    tree %>% 
    predict(mini_test, type = "class") %>% 
    enframe(name = "row_num", value = "guess") %>% 
    bind_cols(select(mini_test, remote, obs))
  preds
}
```

---
# The trouble with trees?

```{r diff-trees, echo=FALSE, fig.asp = NA, fig.align = NA, fig.show="hold", out.width="32%", warning=FALSE, message=FALSE, cache = TRUE}
library(rattle)
fancyRpartPlot(get_boot_trees(1), 
               sub = NULL, 
               palettes = "RdPu")
fancyRpartPlot(get_boot_trees(2), 
               sub = NULL, 
               palettes = "RdPu")
fancyRpartPlot(get_boot_trees(3), 
               sub = NULL, 
               palettes = "RdPu")
```

---
# Bootstrapping + decision trees

.big[Back to rainbows and unicorns!]

---
background-image: url(images/ensembling/ensemble/ensemble.001.jpeg)
background-size: cover

---
background-image: url(images/ensembling/ensemble/ensemble.002.jpeg)
background-size: contain

---
background-image: url(images/ensembling/ensemble/ensemble.003.jpeg)
background-size: contain

---
background-image: url(images/ensembling/ensemble/ensemble.004.jpeg)
background-size: contain

---
background-image: url(images/ensembling/ensemble/ensemble.005.jpeg)
background-size: contain

---
background-image: url(images/ensembling/ensemble/ensemble.006.jpeg)
background-size: contain

---
background-image: url(images/ensembling/ensemble/ensemble.007.jpeg)
background-size: contain

---
background-image: url(images/ensembling/ensemble/ensemble.008.jpeg)
background-size: contain

---
background-image: url(images/ensembling/ensemble/ensemble.009.jpeg)
background-size: contain

---
class: middle, frame, center

# Axiom

There is an inverse relationship between  
model *accuracy* and model *interpretability*.

---
exclude: true

```{r}
plot_tree_resample <- function(rset, id = "Bootstrap01", title = "Sample Variation") {
  lm_train <- function(rset) {
      lm(rainbows ~ unicorns, analysis(rset))
  }
  
  rt_train <- function(rset) {
      rpart::rpart(rainbows ~ unicorns, 
                   data = analysis(rset))
  }
  
  preds <- rset %>% 
      mutate(model = map(splits, lm_train)) %>% 
      mutate(tree = map(splits, rt_train)) %>% 
      mutate(augmented = map(model, augment)) %>% 
      mutate(.fitted_tree = map(tree, predict)) %>% 
      unnest(c(augmented, .fitted_tree))
  
  ggplot(preds, aes(x = unicorns, y = rainbows)) +
      geom_point(size = 3, color = "gray80", alpha = .2) +
      geom_count(data = filter(preds, id == {{ id }}), 
                 color = blue) +
      geom_line(data = filter(preds, id == {{ id }}),
                 aes(x = unicorns, y = .fitted_tree), 
                 colour = pink, size = 2) +
      coord_cartesian(ylim = c(-5, 5), xlim = c(-4, 4)) +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank(),
            plot.title = element_text(hjust = 0.5)) + 
      labs(title = title) +
      scale_size_area(max_size = 7, guide = "none")
}

plot_tree_resamples <- function(rset, title = "Sample Variation") {
  lm_train <- function(rset) {
      lm(rainbows ~ unicorns, analysis(rset))
  }
  
  rt_train <- function(rset) {
      rpart::rpart(rainbows ~ unicorns, 
                   data = analysis(rset))
  }
  
  rset %>% 
      mutate(model = map(splits, lm_train)) %>% 
      mutate(tree = map(splits, rt_train)) %>% 
      mutate(augmented = map(model, augment)) %>% 
      mutate(.fitted_tree = map(tree, predict)) %>% 
      unnest(c(augmented, .fitted_tree)) %>% 
    ggplot(aes(unicorns, rainbows)) +
      geom_point(size = 3, color = "gray80") +
      geom_line(aes(y = .fitted_tree, group = id), 
                colour = pink, alpha=.5, size = 2) +
      coord_cartesian(ylim = c(-5, 5), xlim = c(-4, 4)) +
      theme(axis.text = element_blank(),
            plot.title = element_text(hjust = 0.5)) + 
      labs(title = title)
}

get_training <- function(rset, resample = 1) {
  rset %>% 
    pluck("splits", resample) %>% 
    analysis()
}

# make zero correlation variables
set.seed(100)
x <- rnorm(500)

# shuffle x to get y
set.seed(100)
y <- sample(x, size = 500)

# linear combos of x + y
unicorns <- x + y
rainbows <- x - y
cor(unicorns, rainbows)
uni <- tibble(unicorns = unicorns, rainbows = rainbows)

set.seed(1)
sample_1 <- sample_n(uni, 30)

set.seed(1)
boots <- bootstraps(sample_1, times = 25)

set.seed(1)
big_samples <- mc_cv(uni, prop = 0.6, times = 25)

set.seed(1)
big_boots <- bootstraps(get_training(big_samples, 1), times = 25) 
```

---

```{r tree-boot-1, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resample(boots, id = "Bootstrap25", title = "Tree")
```

---

```{r tree-boot-2, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resample(boots, id = "Bootstrap24", title = "Tree")
```

---

```{r tree-boot-3, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resample(boots, id = "Bootstrap23", title = "Tree")
```

---

```{r tree-boot-4, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resample(boots, id = "Bootstrap22", title = "Tree")
```

---

```{r tree-boot-5, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resample(boots, id = "Bootstrap21", title = "Tree")
```

---

```{r tree-boot-all, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resamples(rset = boots, title = "Bootstrapped Trees") 
```

---

```{r tree-bag-all, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resamples(rset = boots, title = "Bagged Trees") +
  stat_summary(aes(y = .fitted_tree), fun.y = "mean", 
               colour = "navy", size = 2, geom = "line")
```

---

```{r tree-boot-big-all, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resamples(rset = big_boots, title = "Bootstrapped Trees")
```

---

```{r tree-bag-big-all, echo = FALSE, fig.width = 8, out.width = "90%", message = FALSE, warning = FALSE}
plot_tree_resamples(rset = big_boots, title = "Bagged Trees") +
  stat_summary(aes(y = .fitted_tree), fun.y = "mean", 
               colour = "navy", size = 2, geom = "line")
```

---
# `rand_forest()`

.big[Specifies a random forest model]

```{r eval = FALSE}
rand_forest(mtry = 4, trees = 500, min_n = 1)
```

--

*either* mode works!

---
# `rand_forest()`

.big[Specifies a random forest model]

```{r eval = FALSE}
rand_forest(
  mtry = 4,     # predictors seen at each node
  trees = 500,  # trees per forest
  min_n = 1,    # smallest node allowed
)
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Create a new model spec called `rf_spec`, which will learn an ensemble of classification trees from our training data using the **ranger** package. 

Compare the metrics of the random forest to your two single tree models (vanilla and big)- which predicts the test set better?

*Hint: you'll need https://www.tidymodels.org/find/parsnip/*

```{r echo=FALSE}
countdown(minutes = 5)
```

---
class: your-turn

.panelset[
.panel[.panel-name[Random Forest]
```{r fit-rf, cache = TRUE}
rf_spec <-
  rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

set.seed(100)
rf_mod <- fit_resamples(rf_spec,
                        remote ~ .,
                        resamples = so_folds)

rf_preds <- rf_mod %>%
  collect_metrics()
```
]

.panel[.panel-name[Performance]
```{r}
rf_preds
```

```{r include = FALSE}
rf_metrics <- rf_preds %>%
  select(.metric, .estimator, mean)
```
]

.panel[.panel-name[Comparison]
.pull-left[
.big[**Vanilla Decision Tree**]
```{r echo=FALSE}
vt_metrics
```


.big[**Big Decision Tree**]
```{r echo=FALSE}
bt_metrics
```
]

.pull-right[
.big[**Random Forest**]
```{r echo=FALSE}
rf_metrics
```
]
]
]

---
# `mtry`

.big[The number of predictors that will be randomly sampled at each split when creating the tree models.]

```{r eval = FALSE}
rand_forest(mtry = 4)
```

**ranger** default = `floor(sqrt(num_predictors))`

---
---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

.big[Challenge: make 4 more random forest model specs, ecah using 4, 8, 12, and 20 variables at each split. Which value maximizes the area under the ROC curve?]

```{r echo = FALSE}
countdown(minutes = 5)
```

---
class: your-turn

.panelset[
.panel[.panel-name[mtry = 4]
```{r rf4-sol, cache = TRUE}
rf4_spec <- rf_spec %>%
  set_args(mtry = 4)

set.seed(100)
fit_resamples(rf4_spec, remote ~ .,
              resample = so_folds) %>%
  collect_metrics()
```
]

.panel[.panel-name[mtry = 8]
```{r rf8-sol, cache = TRUE}
rf8_spec <- rf_spec %>%
  set_args(mtry = 8)

set.seed(100)
fit_resamples(rf8_spec, remote ~ .,
              resample = so_folds) %>%
  collect_metrics()
```
]

.panel[.panel-name[mtry = 12]
```{r rf12-sol, cache = TRUE}
rf12_spec <- rf_spec %>%
  set_args(mtry = 12)

set.seed(100)
fit_resamples(rf12_spec, remote ~ .,
              resample = so_folds) %>%
  collect_metrics()
```
]

.panel[.panel-name[mtry = 20]
```{r rf20-sol, cache = TRUE}
rf20_spec <- rf_spec %>%
  set_args(mtry = 20)

set.seed(100)
fit_resamples(rf20_spec, remote ~ .,
              resample = so_folds) %>%
  collect_metrics()
```
]
]

---
class: middle, center

```{r rf-tune, cache = TRUE, include=FALSE}
rf_rec <- recipe(remote ~ ., data = so_train)
rf_tune <-
  rand_forest(mtry = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_tune)

set.seed(100)
all_rfs <- tune_grid(
  rf_wf,
  resamples = so_folds,
  grid = expand_grid(mtry = c(4, 8, 12, 20))
)
```

```{r mtry-tune, echo=FALSE, out.width = '90%',}
all_rfs %>%
  collect_metrics() %>%
  ggplot(aes(x = mtry, y = mean)) +
  facet_wrap(~.metric, nrow = 1, scales = "free_y") +
  geom_point(color = blue) +
  geom_line(color = light_blue, lty = 2) +
  labs(x = "# Randomly selected predictors")
```

---
```{r treebag, cache = TRUE}
treebag_spec <-
  rand_forest(mtry = .preds()) %>% #<<
  set_engine("ranger") %>% 
  set_mode("classification")

set.seed(100)
fit_resamples(treebag_spec,
              remote ~ ., 
              resamples = so_folds) %>% 
  collect_metrics()
```

---
# `.preds()`

.big[The number of columns in the data set that are associated with the predictors prior to dummary variable creation.]

```{r eval = FALSE}
rand_forest(mtry = .preds())
```

--

https://parsnip.tidymodels.org/reference/descriptors.html

---
.pull-left[

.big[**Vanilla Decision Tree**]
```{r echo=FALSE}
vt_metrics
```

.big[**Big Decision Tree**]
```{r echo=FALSE}
bt_metrics
```
]

.pull-right[
.big[**Random Forest**]
```{r echo=FALSE}
rf_metrics
```

.big[**Bagging**]
```{r tb-metrics, cache = TRUE, echo=FALSE}
set.seed(100)
tb_metrics <- fit_resamples(treebag_spec,
                            remote ~ ., 
                            resamples = so_folds) %>% 
  collect_metrics()
tb_metrics
```
]

---
class: middle, frame

# .center[To specify a model with parsnip]

.right-column[

.fade[

1\. Pick a .display[model]

]

2\. Set the .display[engine]

.fade[

3\. Set the .display[mode] (if needed)
]
]

---
# `set_engine()`

Adds to a model an R package to train the model.

```{r eval=FALSE}
spec %>% set_engine(engine = "ranger", ...)
```

---
# `set_engine()`

Adds to a model an R package to train the model.

```{r eval=FALSE}
spec %>% 
  set_engine(
    engine = "ranger", # package name in quotes
    ...                # optional arguments to pass to function
    )
```

---
.fade[

# `set_engine()`

Adds to a model an R package to train the model.
]


```{r eval=FALSE}
rf_imp_spec <-
  rand_forest(mtry = 4) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")
```

---
```{r fit-vip, cache = TRUE}
rf_imp_spec <-
  rand_forest(mtry = 4) %>% 
  set_engine("ranger", importance = 'impurity') %>% 
  set_mode("classification")

imp_fit <- fit(rf_imp_spec,
               remote ~ .,
               data = so_train) 
```

---
```{r show-imp-fit, dependson = "fit-vip", cache = TRUE}
imp_fit
```

---
# `vip`

.big[Plot variable importance]

.center[
```{r vip-website, echo=FALSE, out.width = "80%"}
knitr::include_url("https://koalaverse.github.io/vip/index.html")
```
]

---
# `vip()`

.big[Plot variable importance scores for the predictors in a model.]

```{r vip-example, eval = FALSE}
vip(object, geom = "point", ...)
```

---
# `vip()`

.big[Plot variable importance scores for the predictors in a model.]

```{r vip-example2, eval = FALSE}
vip(object,          # fitted model object
    geom = "point",  # one of "col", "point", "boxplot", "violin"
    ...
    )
```

---
```{r imp-plot, out.width = "80%", fig.align = "center"}
vip(imp_fit, geom = "point")
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Make a new model spec called `treebag_imp_spec` to fit a bagged classification tree model. Set the variable `importance` mode to "permutation". Plot the variable importance- which variable was the most important?

```{r yt-treebag-imp-cd, echo=FALSE}
countdown(minutes = 5)
```

---
class: your-turn

.panelset[
.panel[.panel-name[Code]
```{r treebag-vip, fig.show = "hide", cache = TRUE}
treebag_imp_spec <-
  rand_forest(mtry = .preds()) %>% 
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("classification")

imp_fit <- 
  fit(treebag_imp_spec,
      remote ~ ., 
      data = so_train) 

vip(imp_fit)
```
]

.panel[.panel-name[Plot]
```{r plot-imp-sol, echo = FALSE, out.width = "80%"}
include_graphics(fig_chunk("treebag-vip", "png"))
```
]
]

---
class: title-slide, center

# `r rmarkdown::metadata$title`

```{r closing-hex, echo = FALSE, out.width = "18%"}
include_graphics("images/ensembling/hex-group.png")
```

## `r rmarkdown::metadata$subtitle`

### `r rmarkdown::metadata$author`

#### [`r params$class_link`](`r params$class_link`) &#183; [`r params$site_link`](`r params$site_link`)

.footer-license[*Tidy Data Science with the Tidyverse and Tidymodels* is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).]
