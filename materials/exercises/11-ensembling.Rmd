---
title: "Ensembling"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(vip)

# read in the data
stackoverflow <- read_rds(here::here("materials/data/stackoverflow.rds"))

# split the data
set.seed(100) # Important!
so_split <- initial_split(stackoverflow, strata = remote)
so_train <- training(so_split)
so_test  <- testing(so_split)
```


# Your turn 1

Fill in the blanks to return the accuracy and ROC AUC for the vanilla decision tree model.

```{r}
vanilla_tree_spec <-
  decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

set.seed(100)
so_folds <- vfold_cv(so_train, strata = remote)

_________(_________,
          remote ~ ., 
          resamples = so_folds) %>% 
  _________
```


# Your Turn 2

Create a new classification tree model spec; call it `big_tree_spec`. Set the cost complexity to `0`, and the minimum number of data points in a node to split to be `1`. 

Compare the metrics of the big tree to the vanilla tree- which one predicts the test set better?

```{r}

```


# Your Turn 3

Create a new model spec called `rf_spec`, which will learn an ensemble of classification trees from our training data using the **ranger** package. 

Compare the metrics of the random forest to your two single tree models (vanilla and big)- which predicts the test set better?

*Hint: you'll need https://tidymodels.org/find/parsnip*

```{r}

```


# Your Turn 4

Challenge: Make 4 more random forest model specs, each using 4, 8, 12, and 20 variables at each split. Which value maximizes the area under the ROC curve?

```{r}

```

```{r}

```

```{r}

```

```{r}

```


# Your Turn 5

Make a new model spec called `treebag_imp_spec` to fit a bagged classification tree model. Set the variable `importance` mode to "permutation". Plot the variable importance- which variable was the most important?

```{r}

```
